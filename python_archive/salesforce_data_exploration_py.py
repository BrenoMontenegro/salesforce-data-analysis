# -*- coding: utf-8 -*-
"""salesforce_data_exploration

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G36R1Wjx_MnVDmIbwUgvaM6OBB3n1X4O

# Salesforce Data Analysis – Internship Selection Project
### Candidate: Breno Montenegro
### Last Updated: December 12, 2025 (UTC)
---

## 1. Project Overview
This notebook was developed as part of the selection process for a data internship position.  
The goal is to analyze anonymized Salesforce datasets, perform exploratory analysis, derive insights, and demonstrate analytical thinking, data manipulation skills, and communication clarity.

---

## 2. Datasets Used
Two anonymized datasets were provided:

### **a) Accounts Dataset (`accounts_anonymized.json`)**
Contains information about customer accounts, including:
- Account ID
- Country
- Industry sector
- Account creation date
- Account anonymized name

### **b) Support Cases Dataset (`support_cases_anonymized.json`)**
Contains support tickets registered by customers, with fields such as:
- Case ID
- Linked Account ID
- Product involved
- Case status and severity
- Category, type, and reason
- Creation and closure dates

These datasets can be joined via the field **`account_sfid`**, enabling customer-level aggregated analysis.

---

## 3. Objectives
- Load and inspect both datasets  
- Merge datasets when appropriate (Accounts × Support Cases)  
- Perform exploratory data analysis (EDA)  
- Identify trends in support requests, industries served, and customer distribution  
- Highlight patterns in case status, severity, and product usage  
- Generate visualizations to support insights  
- Summarize findings and potential business implications

---

## 4. Tools and Environment
This notebook runs on **Google Colab** using:
- Python
- Pandas  
- NumPy  
- Matplotlib / Seaborn / Plotly  
- Scikit-learn for preprocessing or clustering
- SQLite  

---

## 5. Notebook Structure
1. Importing libraries  
2. Loading datasets  
3. Dataset inspection and cleaning  
4. Exploratory Data Analysis (EDA)  
5. SQL Processing and KPI Aggregation
6. Visualizations  
7. Business Insights  
8. Recommendations
9. Conclusions
---

> *This structure ensures clarity, reproducibility, and alignment with industry standards for data analysis workflows.*

> ## 1. Importing Libraries

The following libraries will be used throughout the analysis for data manipulation, visualization, and exploratory analysis. Additional libraries may be included later as needed.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files

print("Libraries imported successfully.")

"""> ## 2. Loading the Data

In this section, both anonymized Salesforce datasets are loaded into Pandas DataFrames.  
The files include account-level information and support case records, which will be explored and analyzed throughout the project.

The datasets are provided in JSON format and can be read directly using "pd.read_json()".

"""

uploaded = files.upload()

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 50)

sns.set(style="whitegrid")

accounts_df = pd.read_json("accounts_anonymized.json")
cases_df = pd.read_json("support_cases_anonymized.json")

print("Accounts dataset shape:", accounts_df.shape)
print("Support cases dataset shape:", cases_df.shape)

accounts_df.head()
cases_df.head()

"""> ## 3. Dataset Inspection and Initial Cleaning

In this section, we perform an initial inspection of both datasets, checking their structure, data types, missing values, and general consistency.  
This step ensures the datasets are ready for proper exploratory data analysis (EDA) and prevents unexpected issues during the analysis workflow.

### 3.1 General View
"""

print("Accounts DataFrame:")
display(accounts_df.head())

print("\nSupport Cases DataFrame:")
display(cases_df.head())

"""### 3.2 Data structure (columns and types)"""

print("Accounts info:")
accounts_df.info()

print("\nSupport Cases info:")
cases_df.info()

"""### 3.3 Null Values verification"""

print("Missing values in Accounts dataset:")
print(accounts_df.isnull().sum())

print("\nMissing values in Support Cases dataset:")
print(cases_df.isnull().sum())

"""### 3.4 Standardizing Column Names

Standardizing column names is an important step to ensure consistency across the datasets.  
By converting all column names to a uniform format (such as lowercase), we avoid issues caused by case sensitivity, improve code readability, and make future data manipulation more reliable and less error-prone.
"""

accounts_df.columns = accounts_df.columns.str.lower()
cases_df.columns = cases_df.columns.str.lower()

"""Standardizing column names ensures consistency across Python and SQL operations.

### 3.5 Converting Date Columns

Converting date columns into proper datetime formats is essential for accurate analysis.  
This step ensures that operations such as filtering, sorting, computing time differences, and extracting components (year, month, day) work correctly.  
It also prevents inconsistencies that may arise when dates are stored as strings instead of structured datetime objects.
"""

date_cols_accounts = ["account_created_date"]
date_cols_cases = ["case_created_date", "case_closed_date"]

for col in date_cols_accounts:
    if col in accounts_df.columns:
        accounts_df[col] = pd.to_datetime(accounts_df[col], errors="coerce")

for col in date_cols_cases:
    if col in cases_df.columns:
        cases_df[col] = pd.to_datetime(cases_df[col], errors="coerce")

"""> ## 4. Exploratory Data Analysis (EDA)

This section performs an exploratory analysis of the Accounts and Support Cases datasets.  
The goal is to understand the structure, distribution, relationships, and potential issues within the data before moving into SQL-based processing.

The EDA focuses on:
- Summary statistics
- Distributions of key fields
- Categorical value frequencies
- Relationships between account and case fields
- Identifying potential patterns or anomalies

### 4.1. Summary Statistics (numeric columns)
"""

print("Accounts dataset — summary statistics:")
display(accounts_df.describe())

print("\nSupport Cases dataset — summary statistics:")
display(cases_df.describe())

"""### 4.2. Categorical Column Exploration"""

accounts_cat = accounts_df.select_dtypes(include=['object']).columns
cases_cat = cases_df.select_dtypes(include=['object']).columns

print("Accounts categorical columns:", accounts_cat)
print("Support Cases categorical columns:", cases_cat)

for col in accounts_cat:
    print(f"\nValue counts for {col} (Accounts):")
    print(accounts_df[col].value_counts().head(10))

for col in cases_cat:
    print(f"\nValue counts for {col} (Support Cases):")
    print(cases_df[col].value_counts().head(10))

"""### 4.3. Missing Values Visualization (heatmap)"""

plt.figure(figsize=(12, 5))
sns.heatmap(accounts_df.isnull(), cbar=False)
plt.title("Missing Values — Accounts Dataset")
plt.show()

plt.figure(figsize=(12, 5))
sns.heatmap(cases_df.isnull(), cbar=False)
plt.title("Missing Values — Support Cases Dataset")
plt.show()

"""### 4.4. Case Status Distribution"""

plt.figure(figsize=(10,5))
cases_df["case_status"].value_counts().plot(kind='bar')
plt.title("Support Case Status Distribution")
plt.xlabel("Status")
plt.ylabel("Count")
plt.show()

"""### 4.5. Cases Over Time (Creation Date)"""

cases_over_time = cases_df.groupby(cases_df['case_created_date'].dt.date).size()

plt.figure(figsize=(12,5))
cases_over_time.plot()
plt.title("Support Cases Created Over Time")
plt.xlabel("Date")
plt.ylabel("Number of Cases")
plt.show()

"""> ## 5. SQL Processing and KPI Aggregation

This section uses SQL (via SQLite) to join the datasets and compute key business metrics.  
The SQL environment allows us to perform relational operations, aggregations, and transformations that mirror real-world analytics workflows.

The main goals of this section include:
- Creating summary statistics using SQL
- Joining Accounts and Support Cases
- Generating KPIs such as number of cases per account, cases by product, status distributions, and industry-level metrics
- Preparing aggregated tables for later visualization

### 5.1 Creating SQLite In-Memory Database
"""

import sqlite3

conn = sqlite3.connect(":memory:")

# Export Pandas DataFrames to SQL tables
accounts_df.to_sql("accounts", conn, index=False, if_exists="replace")
cases_df.to_sql("support_cases", conn, index=False, if_exists="replace")

print("SQLite in-memory database created and tables loaded successfully.")

"""### 5.2 — Join Accounts and Cases"""

query_join = """
SELECT
    a.account_sfid,
    a.account_name,
    a.account_country,
    a.account_industry,
    c.case_sfid,
    c.case_status,
    c.case_severity,
    c.case_product,
    c.case_created_date,
    c.case_closed_date
FROM accounts a
LEFT JOIN support_cases c
    ON a.account_sfid = c.account_sfid;
"""

accounts_cases_joined = pd.read_sql_query(query_join, conn)
accounts_cases_joined.head()

"""### 5.3 — KPI: Cases Status Distribution"""

query_status = """
SELECT
    case_status,
    COUNT(*) AS total_cases
FROM support_cases
GROUP BY case_status
ORDER BY total_cases DESC;
"""

cases_by_status = pd.read_sql_query(query_status, conn)
cases_by_status

"""### 5.4 — KPI: Cases by Product"""

query_cases_by_product = """
SELECT
    case_product,
    COUNT(*) AS total_cases
FROM support_cases
GROUP BY case_product
ORDER BY total_cases DESC;
"""

cases_by_product = pd.read_sql_query(query_cases_by_product, conn)
cases_by_product

"""### 5.5 — KPI: Cases by Industry (JOIN)"""

query_cases_by_industry = """
SELECT
    a.account_industry,
    COUNT(c.case_sfid) AS total_cases
FROM accounts a
LEFT JOIN support_cases c
    ON a.account_sfid = c.account_sfid
GROUP BY a.account_industry
ORDER BY total_cases DESC;
"""

cases_by_industry = pd.read_sql_query(query_cases_by_industry, conn)
cases_by_industry

"""### 5.6 — KPI: Average Time to Close Cases"""

query_time_to_close = """
SELECT
    AVG(
        JULIANDAY(case_closed_date) - JULIANDAY(case_created_date)
    ) AS avg_days_to_close
FROM support_cases
WHERE case_closed_date IS NOT NULL;
"""

avg_time_to_close = pd.read_sql_query(query_time_to_close, conn)
avg_time_to_close

"""### 5.7 — KPI: Cases Created per Day"""

query_cases_per_day = """
SELECT
    DATE(case_created_date) AS day,
    COUNT(*) AS total_cases
FROM support_cases
GROUP BY day
ORDER BY day;
"""

cases_per_day = pd.read_sql_query(query_cases_per_day, conn)
cases_per_day.head()

"""> ## 6. Visualizations

### 6.1 Cases per Industry (Bar Chart)
"""

industry_cases = pd.read_sql("""
SELECT
    a.account_industry AS industry,
    COUNT(c.case_sfid) AS total_cases
FROM accounts a
LEFT JOIN support_cases c
    ON a.account_sfid = c.account_sfid
GROUP BY a.account_industry
ORDER BY total_cases DESC
""", conn)

plt.figure(figsize=(12,6))
sns.barplot(data=industry_cases, x="total_cases", y="industry")
plt.title("Total Support Cases by Industry")
plt.xlabel("Total Cases")
plt.ylabel("Industry")
plt.show()

"""### 6.2 Average Resolution Time (Horizontal Bar Chart)"""

resolution = pd.read_sql("""
SELECT
    a.account_industry AS industry,
    AVG(julianday(c.case_closed_date) - julianday(c.case_created_date)) AS avg_days
FROM accounts a
JOIN support_cases c
    ON a.account_sfid = c.account_sfid
WHERE c.case_closed_date IS NOT NULL
GROUP BY a.account_industry
ORDER BY avg_days DESC;
""", conn)

plt.figure(figsize=(12,6))
sns.barplot(data=resolution, x="avg_days", y="industry")
plt.title("Avg Resolution Time (Days) by Industry")
plt.xlabel("Days")
plt.ylabel("Industry")
plt.show()

"""### 6.3 Case Status Distribution (Pie or Bar Chart)"""

status = pd.read_sql("""
SELECT
    case_status,
    COUNT(*) AS total_cases
FROM support_cases
GROUP BY case_status
""", conn)

plt.figure(figsize=(8,6))
sns.barplot(data=status, x="case_status", y="total_cases")
plt.title("Case Status Distribution")
plt.xlabel("Status")
plt.ylabel("Count")
plt.xticks(rotation=45)
plt.show()

"""> ## 7. Business Insights

Based on the SQL KPIs and exploratory analysis, several meaningful insights emerge:

### 1. Support Case Volume Concentration
A small portion of accounts generates most support cases.  
This suggests that a subset of customers may require additional onboarding, training, or product support to reduce ticket volume.

### 2. Case Status and Resolution Behavior
The distribution of case statuses reveals clear patterns in how cases flow through the support process.  
If a high share of cases remains open or unresolved, this may indicate bottlenecks in the support pipeline.

### 3. Product-Specific Case Trends
Some products generate significantly more cases than others.  
This points to potential usability issues or gaps in documentation affecting those product lines.

### 4. Industry-Level Support Demand
Industries differ in the number of cases submitted, suggesting certain verticals require more frequent assistance.  
This can inform targeted support resources or industry-focused improvements.

### 5. Time to Resolution
The average time to close cases provides a general indicator of support team efficiency.  
If resolution times are long, prioritization strategies or workflow adjustments may be needed.

Overall, the data highlights which customer segments, products, and operational areas deserve attention and where improvements can maximize support

1. Certain industries generate significantly higher case volume, indicating greater product adoption or support complexity.  
2. "Duplicate" and "Undefined" cases appear frequently, suggesting potential inefficiencies in ticket triage.  
3. A few products (e.g., GVD) drive most support demand, indicating areas for proactive documentation or training.  
4. Resolution times vary widely across industries, which may highlight differences in customer maturity or support workflow.

> ## 8. Recommendations

1. Improve support triage to reduce Duplicate cases, increasing team efficiency.  
2. Provide targeted training or documentation for high-volume products to reduce future ticket load.

> ## 9. Conclusions

This analysis combined Python-based data exploration with SQL-driven processing to extract meaningful insights from Salesforce account and support case data.  
The workflow included cleaning and standardizing the datasets, computing key performance indicators through SQL queries, and interpreting patterns relevant to customer support operations.

Across the exploration, several themes emerged: support demand is unevenly distributed across accounts and industries; certain products drive more case activity; and resolution times highlight opportunities for operational improvement. These findings offer valuable direction for enhancing customer experience and optimizing support resources.

Overall, the project demonstrates a structured analytical approach, blending technical execution with business-oriented interpretation. The methodologies used here can be extended with additional metrics, dashboards, or predictive models to further support decision-making.

*This project demonstrates a complete workflow integrating Python, SQL, and visualization to derive actionable insights from Salesforce account and case data. The findings highlight key operational opportunities, particularly around triage efficiency and product-specific support patterns.*
"""